## ~~~~~~~~~~~~~~~
#  Deep Q Learning
## ~~~~~~~~~~~~~~~

''' 

Depict game we want to play as a black box. It has a given state, you give it an action and it returns you a reward that depends on both the action and the state. The state changes and you repeat until the game ends (terminal state).

Idea is that the network predicts the Q-value of each action given a state. The Q-value is a sort of value/utility of the action (a) given the state (s):
    Q(s, a) = Sum_t y^t * r_t = r(s, a) + y * max_a'{Q(s', a')}
where r(s, a) is the immediate reward of action a in state s (Bellman equation).

We can thus pick actions by running the network on a given state and choosing the action with highest Q(s, a).

While playing game, we update the training examples and retrain the network to optimise the prediction. We add some random behaviour in order to allow the player to 'explore' the game (otherwise it repeats the actions it thinks are optimal over and over and can get stuck in a local minimum). The idea is that this iteration should make the network's estimate of Q(s, a) near the true Q(s, a).

Steps for running the learning. 
1. Initialise the class instances, including the neural network (input: dim(state space), output: dim(action space));
2. Run game. At each turn:
   a) Run a lottery to see if action will be random or if it will be based on running the network with current weights and choosing action with greatest output (Q-value);
   b) Store old state, action, reward and new state (inc. if it crashed) into the memory: (s, a, r, s', c);
   c) Train network on a single training example generated by:
       - running network on s' (store that) and on s;
       - updating result from running on s corresponding to action a to be r + y * max_a'{Q(s', a')}, store this new result as y
       - train network on single example (x = s, y = y)
3. Train network on larger sample of memory:
   a) Extract batch from memory
   b) Rowwise on memory batch, do the first two steps described in 2c above  to generate a larger training sample
   c) Train network on this sample
4. Go back to step 2. Run game again a set number of times to allow network to learn.

Add in:
 - lottery that decides if move is random or not starts off always giving random and slowly gives less and less
 - variable (slowing) learning rate

'''

import types, random, numpy as np, math
import WillyNet as WN
from sklearn.neural_network import MLPClassifier, MLPRegressor

import time, multiprocessing as mp
from pathos.multiprocessing import ProcessPool

use_WillyNet = True


## Q-learning player
## ~~~~~~~~~~~~~~~~~

class QPlayer(object):
    ''' Implements things like memory, the deep Q network and training on a batch from memory. '''
    
    def __init__(self, disc_rate, max_memory_len, memory_props):
        
        self.memory = Memory(max_memory_len, memory_props)
        self.disc_rate = disc_rate
    
    def start_network(self, shape):
        
        self.num_inputs = shape[0]
        self.num_outputs = shape[-1]
        
        if use_WillyNet:
            self.network = WN.WillyNet(shape, 
                                       problem = 'regression')
        else:
            self.network = MLPRegressor(
                        hidden_layer_sizes = tuple(shape[1:-2]),
                        activation = 'relu',
                        solver = 'sgd',
                        learning_rate = 'constant',
                        learning_rate_init = 0.001,
                        batch_size = 150,
                        alpha = 0.0001)
        
    def load_network(self, filename):
        
        self.rule_network = WN.WillyNet.load(filename)
        self.num_inputs = self.network.shape[0]
        self.num_outputs = self.network.shape[-1]
        
    def get_move(self, state, random_state, legal_moves):
        ''' Selects random move if random_move == True. Otherwise, runs network to select move with the highest expected (Q-)value. '''
        
        # No randomness
        if not random_state:
            
            if use_WillyNet:
                preds = self.network.forward_prop([state])[0]
            else:
                preds = self.network.predict([state])[0]
            
            choices = np.zeros(self.num_outputs)
            for move in legal_moves:
                choices[move] = 1
            
            assert type(preds) == np.ndarray and type(choices) == np.ndarray
            
            return np.argmax(preds * choices)
            
        # Random choice based on predictions
        else:
            
            return np.random.choice(legal_moves)
            
    def store(self, state, action, reward, new_state, crash, legal_moves):
        
        self.memory.append([state, action, reward, new_state, crash, legal_moves], reward)
        
    def retrieve(self, ind = None):
        
        if ind is None:
            return self.memory.memory
        
        return self.memory[ind]
    
    def train(self, batch_size, l_rate, 
              use_last = True, reg_rate = 0, mom_rate = 0,
              verbose = False):
        ''' Trains the network on a batch drawn randomly from memory. If use_last == True, assures the latest training example is included in the batch. 
           Prepares batch, rowwise, by:
            - running network on s' and on s;
            - use as target the result from running on s with the following corrections:
              a) Q-value corresponding to a -> r + y * max_a'{Q(s', a')};
              b) Q-value corresponding to illegal moves -> 0.
            '''
        
        # Gen batch for training
        if (batch_size == 1 or len(self.memory) == 1) and use_last:
            batch = np.array([self.memory[-1]])
        elif use_last:
            batch = np.random.permutation(self.memory[:-1])[:batch_size-1]
            batch = np.append(batch, [self.memory[-1]], axis = 0)
        else:
            batch = np.random.permutation(self.memory)[:batch_size]
        
        # Training examples
        examples = np.array(list(batch[:, 0]))
        
        # Update targets for training examples in batch
        if use_WillyNet:
                targets = self.network.forward_prop(examples)
                # legal_moves = self.rule_network.predict(examples, 'binary')
        else:
            targets = self.network.predict(examples)
        
        old_fp = np.copy(targets)
        
        for u, unit in enumerate(batch):
            
            # Inds: 0 - state, 1 - action, 2 - reward, 3 - new state, 4 - crash, 5 - legal moves
            
            if use_WillyNet:
                new_pred = self.network.forward_prop([unit[0]])
            else:
                new_pred = self.network.predict([unit[0]])
            
            max_new_pred = np.max(new_pred)
            
            targets[u, unit[1]] = unit[2] + \
                (not unit[4]) * self.disc_rate * max_new_pred
            
            legal_moves = np.array([1 if i in unit[5] else 0 for i in range(self.num_outputs)])
            targets[u] *= legal_moves
            
        # Given examples and targets, train
        if use_WillyNet:
                self.network.train(X = examples, y = targets,
                           num_iterations = 1, batch_size = -1,
                           l_rate = l_rate,
                           reg_rate = reg_rate, mom_rate = mom_rate)
        else:
            self.network.fit(examples, targets)
    
        # Print stuff
        if use_WillyNet:
            new_fp = self.network.forward_prop(examples)
        else:
            new_fp = self.network.predict(examples)
        
        if verbose:
            
            print('  RMSE:', np.mean((old_fp - targets)**2)**0.5, 
                  '->', np.mean((old_fp - targets)**2)**0.5)
            
            if use_WillyNet:
                print('  Cost:', self.network.cost(old_fp, targets),
                        '->', self.network.cost(new_fp, targets))
            
#            # If nan, print a lot of things to diagnose problems
#            if math.isnan(np.mean(new_fp - old_fp)):
#                
#                print('\n\n\n', \
#                      '  PROBLEM! nan difference between new and old predictions!\n\n')
#                
#                while True:
#                    command = input('Problem! Input your command (inside train) (input "break" to continue):\n  ')
#                    exec(command)
                    


## Memory
## ~~~~~~

class Memory(object):
    
    def __init__(self, max_len, proportions = None):
        
        self.memory = []
        self.max_len = max_len
        
        self.proportions = proportions
        self.types = []
        self.counts = {}
        
    def is_full(self):
        
        return len(self.memory) == self.max_len
    
    def __getitem__(self, i):
        
        return self.memory[i]
    
    def __len__(self):
        
        return len(self.memory)
    
    def append(self, item, item_type = None):
        
        # Add to memory
        self.memory.append(item)
        
        if self.proportions is not None:
            
            self.types.append(item_type)
            self.counts[item_type] = self.counts.get(item_type, 0) + 1

            if self.counts[item_type] > self.proportions[item_type] * self.max_len:
                inds = np.random.permutation(range(len(self)-1))
                i = 0
                while self.types[inds[i]] != item_type:
                    i += 1
                del self.memory[inds[i]]
                del self.types[inds[i]]
    
    def clear(self):
        
        self.memory = []
        self.types = []
        self.counts = {}


## RL routine
## ~~~~~~~~~~

class RLRoutine(object):
    ''' A class for the Reinforcement learning routine - just a wrapper of a QPlayer and a ComputerGame. Stores the agent and history of games as attributes and has two methods: one for learning and one for playing (in a way humans can visualise). '''
    
    def __init__(self, epsilon, disc_rate, 
                 max_memory_len, memory_props,
                 shape, filename = None):
        
        # Initialise QPlayer class instance
        self.qplayer = QPlayer(disc_rate, max_memory_len, memory_props)
#        if filename is None:
#                self.qplayer.start_network(shape)
#            else:
#                self.qplayer.load_network(filename)
        
        self.qplayer.start_network(shape)
    
        # Store parameters
        self.epsilon = epsilon          # Function of the epoch
        self.disc_rate = disc_rate
        self.max_memory_len = max_memory_len
        self.memory_props = memory_props
    
    def play_cgame(self, epsilon):
        ''' Play one game with QPlayer. Chooses action following epsilon greedy policy each turn and stores each turn in memory. 
            Returns tuple containing the ComputerGame instance, the number of turns played and the number of illegal moves chosen. '''
        
        cgame = self.cgame_class()
        
        # First turn - remove d4
        action = self.cgame_class.get_inp_out_dims()[1]//2
        cgame.first_turn(action)
        
        new_state = cgame.get_state()
        
        # Loop over next turns
        memory = []
        turns = 0
        illegal_choices = 0
        while not cgame.crash():
            
            # Get action (repeat choice until legal)
            state = new_state
            
            legal = False
            while not legal:
                
                action = self.qplayer.get_move(
                            state,
                            random.random() < epsilon,
                            cgame.legal_moves()
                         )
                
                # Was it legal?
                legal_moves = cgame.legal_moves()
                legal = action in legal_moves
                
                illegal_choices += int(not legal)
                
                # Play turn
                reward = cgame.turn(action)
                new_state = cgame.get_state()
                
                # Store in temp memory
                memory.append([state, action, reward, new_state, cgame.crash() if legal else True, legal_moves])
                
            turns += 1
        
        # Return cgame (has history)
        return (cgame, turns, illegal_choices, memory)
            
    def learn(self, cgame_class,
              epochs, batch_size, l_rate, 
              reg_rate = 0, mom_rate = 0,
              verbose = False,
              n_processes = None):
        ''' Whole RL routine for learning. In a loop:
            1. Run many games in parallel, storing actions in memory but not training.
            2. Train on previous set of games played in parallel.
            3. Every set number of iterations, play a game with no epsilon greedy policy to track progress.
            '''
        
        # Handle arg: cgame_class
        assert isinstance(cgame_class, type), 'Argument cgame_class must be a class (not instance).'
        self.cgame_class = cgame_class
        self.cgame_list = []
        
        # Handle arg: n_processes
        if n_processes is None:
            n_processes = mp.cpu_count() - 2
        
        # Handle arg: verbose
        if verbose is None: verbose == np.Inf
        
        # Play games
        self.turn_list = []
        self.illegal_move_list = []
        
        self.det_turn_list = []
        self.det_illegal_move_list = []
        
        start_time = time.time()
        
        epoch = 0
        while epoch < epochs:
            
            print('\nDoing epoch', epoch, end = ':\n')
            
            # Play n_processor games in parallel
#            pool = mp.Pool(n_processes)
#            games = [pool.apply(self.play_cgame, 
#                                args=(self.epsilon(epoch),)) \
#                        for x in range(n_processes)]
#            
#            output = [p.get() for p in results]
            
            pool = ProcessPool(n_processes)
            output = pool.map(self.play_cgame, 
                              [self.epsilon(epoch)] * n_processes)
            for o in output:
                self.cgame_list.append(o[0])
                self.turn_list.append(o[1])
                self.illegal_move_list.append(o[2])
                for item in o[3]:
                    self.qplayer.memory.append(item)
            
            print('  Mean turns: %0.1f, with %0.1f illegal moves' %(np.mean([o[1] for o in output]), np.mean([o[2] for o in output])))
            
            # Train
            self.qplayer.train(batch_size = batch_size, 
                               use_last = True, 
                               l_rate = l_rate, 
                               reg_rate = reg_rate,
                               mom_rate = mom_rate,
                               verbose = True)
                
            #self.qplayer.memory.clear()
            
            if epoch % verbose == 0:
                
                verbose_time = time.time()
                cgame, turns, illegal_moves, _ = self.play_cgame(-1)
                
                print('\nDeterministic game, %i mins into training:\n  Played %i turns with %i illegal choices made.' % ((verbose_time - start_time)/60, turns, illegal_moves))
                
                self.det_turn_list.append(turns)
                self.det_illegal_move_list.append(illegal_moves)
            
            epoch += 1
            
            