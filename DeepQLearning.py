## ~~~~~~~~~~~~~~~
#  Deep Q Learning
## ~~~~~~~~~~~~~~~

''' 

Depict game we want to play as a black box. It has a given state, you give it an action and it returns you a reward that depends on both the action and the state. The state changes and you repeat until the game ends (terminal state).

Idea is that the network predicts the Q-value of each action given a state. The Q-value is a sort of value/utility of the action (a) given the state (s):
    Q(s, a) = Sum_t y^t * r_t = r(s, a) + y * max_a'{Q(s', a')}
where r(s, a) is the immediate reward of action a in state s (Bellman equation).

We can thus pick actions by running the network on a given state and choosing the action with highest Q(s, a).

While playing game, we update the training examples and retrain the network to optimise the prediction. We add some random behaviour in order to allow the player to 'explore' the game (otherwise it repeats the actions it thinks are optimal over and over and can get stuck in a local minimum). The idea is that this iteration should make the network's estimate of Q(s, a) near the true Q(s, a).

Steps for running the learning. 
1. Initialise the class instances, including the neural network (input: dim(state space), output: dim(action space));
2. Run game. At each turn:
   a) Run a lottery to see if action will be random or if it will be based on running the network with current weights and choosing action with greatest output (Q-value);
   b) Store old state, action, reward and new state (inc. if it crashed) into the memory: (s, a, r, s', c);
   c) Train network on a single training example generated by:
       - running network on s' (store that) and on s;
       - updating result from running on s corresponding to action a to be r + y * max_a'{Q(s', a')}, store this new result as y
       - train network on single example (x = s, y = y)
3. Train network on larger sample of memory:
   a) Extract batch from memory
   b) Rowwise on memory batch, do the first two steps described in 2c above  to generate a larger training sample
   c) Train network on this sample
4. Go back to step 2. Run game again a set number of times to allow network to learn.

Add in:
 - lottery that decides if move is random or not starts off always giving random and slowly gives less and less
 - variable (slowing) learning rate

'''

import types
import random
import numpy as np

import NeuralNet as NN

## Q-learning player
## ~~~~~~~~~~~~~~~~~

class QPlayer(object):
    ''' Implements things like memory, the deep Q network and training on a batch from memory. '''
    
    def __init__(self, disc_rate, max_memory_len, memory_props):
        
        self.memory = Memory(max_memory_len, memory_props)
        self.disc_rate = disc_rate
    
    ''' I should think of what's the best way to program the network itself because the input (board configuration) will have a lot of structure and symmetries that could be expoited, e.g. spatial relation of pieces in board (suggests CNN) and the rotational symmetry of the board (playing it in any orientation is the same - suggests an analogue of CNN where instead of convolving we convolve four rotations of the board? or something like that...). '''
    def start_network(self, sizes, 
                      act_func = NN.Sigmoid, 
                      cost_func = NN.CrossEntropyCost):
        
        self.num_inputs = sizes[0]
        self.num_outputs = sizes[-1]
        
        self.network = NN.NeuralNet(sizes = sizes,
                                    act_func = act_func,
                                    cost_func = cost_func)
    
    def load_network(self, filename):
        
        self.network = NN.NeuralNet.load(filename)
        self.num_inputs = self.network.shape[0]
        self.num_outputs = self.network.shape[-1]
        
    def get_move(self, state, random_move):#, legal_moves):
        ''' Selects random move if random_move == True. Otherwise, runs network to select move with the highest expected (Q-)value. '''
       
        # Random choice
        if random_move:
            return random.choice(range(self.num_outputs))
        
        # From network
        preds = self.network.forward_prop([state])[0]
        
        move = np.argmax(preds)
        
        #while move not in legal_moves:    
            #preds = np.delete(preds, move)
            #move = np.argmax(preds)
        
        return int(move)
    
    def store(self, state, action, reward, new_state, crash):
        
        self.memory.append([state, action, reward, new_state, crash], reward)
        
    def retrieve(self, ind = None):
        
        if ind is None:
            return self.memory.memory
        
        return self.memory[ind]
    
    ### NOT USED ANYMORE
    @staticmethod
    def states_as_string(states, many = True):
        ''' For use in train. Converts states as list to state as string, eg. [1, 1, 0, ...] -> '110...'. '''
        
        if not many:
            
            string_state = ''
            for i in states:
                string_state += str(i)

            return string_state
        
        else:
            
            return np.array([QPlayer.states_as_string(state, False) for state in states])
    
    ### NOT USED ANYMORE - FASTER VERSION BELOW
    def old_train(self, batch_size, learn_rate, 
              use_last = True, reg = None, reg_rate = None):
        ''' Trains the network on a batch drawn randomly from memory. If use_last == True, assures the latest training example is included in the batch. '''
        
        # Gen batch for training
        if (batch_size == 1 or len(self.memory) == 1) and use_last:
            batch = np.array([self.memory[-1]])
        elif use_last:
            batch = np.random.permutation(self.memory[:-1])[:batch_size-1]
            batch = np.append(batch, [self.memory[-1]], axis = 0)
        else:
            batch = np.random.permutation(self.memory)[:batch_size]
        
        batch = batch.transpose()
        
        # Get training examples
        examples = np.array(list(batch[0]))
        
        # Gen 'labels' for training examples in batch
        # Inds: 0 - state, 1 - action, 2 - reward, 3 - new state, 4 - crash
        string_new_states = self.states_as_string(batch[3], many = True)
        targets = self.network.forward_prop(examples).transpose()
        
        for new_state in set(tuple(x) for x in batch[3]):
            
            string_new_state = self.states_as_string(new_state, False)
            
            new_prediction = self.network.forward_prop([list(new_state)])
            max_new_pred = np.max(new_prediction[0])
            
            for i in range(targets.shape[0]):
                
                targets[i] = targets[i] + \
                             (string_new_states == string_new_state) * \
                             (batch[1] == i) * \
                             (batch[2] + (batch[4] == False) * self.disc_rate * max_new_pred - targets[i])
        
        targets = targets.transpose()
        
        # Given batch of training examples and targets to train, train
        self.network.train(X = examples, y = targets,
                           epochs = 1, batch_size = -1,
                           eta = learn_rate,
                           reg = reg, lmbda = reg_rate)

    def train(self, batch_size, learn_rate, 
              use_last = True, reg = None, reg_rate = None):
        ''' Trains the network on a batch drawn randomly from memory. If use_last == True, assures the latest training example is included in the batch. '''
        
        # Gen batch for training
        if (batch_size == 1 or len(self.memory) == 1) and use_last:
            batch = np.array([self.memory[-1]])
        elif use_last:
            batch = np.random.permutation(self.memory[:-1])[:batch_size-1]
            batch = np.append(batch, [self.memory[-1]], axis = 0)
        else:
            batch = np.random.permutation(self.memory)[:batch_size]
        
        # Training examples
        examples = np.array(list(batch[:, 0]))
        
        # Update targets for training examples in batch
        targets = self.network.forward_prop(examples)
        
        for u, unit in enumerate(batch):
            
            # Inds: 0 - state, 1 - action, 2 - reward, 3 - new state, 4 - crash
            
            new_pred = self.network.forward_prop([unit[0]])
            max_new_pred = np.max(new_pred)
            
            targets[u, unit[1]] = unit[2] + \
                (not unit[4]) * self.disc_rate * max_new_pred
        
        # Given examples and targets, train
        self.network.train(X = examples, y = targets,
                           epochs = 1, batch_size = -1,
                           eta = learn_rate,
                           reg = reg, lmbda = reg_rate)



## Memory
## ~~~~~~

class Memory(object):
    
    def __init__(self, max_len, proportions = None):
        
        self.memory = []
        self.types = []
        
        self.max_len = max_len
        self.last_item = None
        
        self.props = proportions
        self.counts = {}
        
    def is_full(self):
        
        return len(self.memory) == self.max_len
    
    def __getitem__(self, i):
        
        return self.memory[i]
    
    def __len__(self):
        
        return len(self.memory)
    
    def append(self, item, item_type = None):
        
        self.memory.append(item)
        self.types.append(item_type)
        
        self.last_item = item
        self.counts[item_type] = self.counts.get(item_type, 0) + 1
        
        if self.counts[item_type] > self.props[item_type] * self.max_len:
            inds = np.random.permutation(range(len(self.memory)-1))
            i = 0
            while self.types[inds[i]] != item_type:
                i += 1
            del self.memory[inds[i]]
            del self.types[inds[i]]


## RL routine
## ~~~~~~~~~~

class RLRoutine(object):
    ''' A class for the Reinforcement learning routine - just a wrapper of a QPlayer and a ComputerGame. Stores the agent and history of games as attributes and has two methods: one for learning and one for playing (in a way humans can visualise). '''
    
    def __init__(self, epsilon, 
                 disc_rate, max_memory_len, memory_props,
                 sizes, act_func = NN.Sigmoid, cost_func = NN.CrossEntropyCost, 
                 filename = None):
        
        # Initialise QPlayer class instance
        self.qplayer = QPlayer(disc_rate, max_memory_len, memory_props)
        if filename is None:
            self.qplayer.start_network(sizes, act_func, cost_func)
        else:
            self.qplayer.load_network(filename)
        
        # Store parameters
        self.epsilon = epsilon          # Function of the epoch
        self.disc_rate = disc_rate
        self.max_memory_len = max_memory_len
        self.memory_props = memory_props
        
    def learn(self, cgame_class,
              games, batch_size, learn_rate, 
              reg = None, reg_rate = None,
              verbose = False):
        ''' Whole RL routine for learning. 
            1. Run game. At each turn:
               a) Run a lottery to see if action will be random or the actor's network;
               b) Play with that action;
               c) Store old state, action, reward, new state and whether the game crashed in memory: (s, a, r, s', c);
               d) Train network on a small batch generated by sampling from memory and, rowwise:
                   - running network on s' (store that) and on s;
                   - using as target the result from running on s with the Q-value corresponding to a updated to r + y * max_a'{Q(s', a')};
            2. Train network on larger sample of memory using the same method as above;
            3. Go back to step 1. Run game a set number of times to allow network to learn.
        '''
        
        assert isinstance(cgame_class, type), 'Argument cgame must be a class (not instance).'
        self.cgame_class = cgame_class
        self.cgames = []
        
        first = True
        
        print('Starting learning.')
        
        game = 0
        counter = 0
        while game < games:
            
            # Start new game
            self.cgames.append(self.cgame_class())
            cgame = self.cgames[-1]
            
            # First turn
            state = cgame.get_state()
            action = self.cgame_class.get_inp_out_dims()[1]//2 # <- d4   
                #self.qplayer.get_move(state, (random.random() < self.epsilon(game)))
                
            reward = cgame.first_turn(action)
            new_state = cgame.get_state()
            
            self.qplayer.store(state, action, reward, new_state, cgame.crash())
            
#            self.qplayer.train(batch_size = 1, use_last = True, 
#                               learn_rate = learn_rate, 
#                               reg = reg, reg_rate = reg_rate)
            
            turns = 1
            counter = 0
            
            # Following turns
            while not cgame.crash():
                
                state = new_state
                action = self.qplayer.get_move(state, (random.random() < self.epsilon(game)))#, cgame.legal_moves())
                
                counter += 1
                
                while action not in cgame.legal_moves():
                    
                    reward = cgame.turn(action, crash_if_invalid = False)
                    self.qplayer.store(state, action, reward, state, True)
                    
                    self.qplayer.train(batch_size = 1, use_last = True,
                                       learn_rate = learn_rate, 
                                       reg = reg, reg_rate = reg_rate)
                    
                    action = self.qplayer.get_move(state, (random.random() < self.epsilon(game)))#, cgame.legal_moves())
                    
                    counter += 1
                
                reward = cgame.turn(action)
                
                new_state = cgame.get_state()
                
                self.qplayer.store(state, action, reward, new_state, cgame.crash())
                
                self.qplayer.train(batch_size = 1, use_last = True, 
                                   learn_rate = learn_rate, 
                                   reg = reg, reg_rate = reg_rate)
                
                turns += 1
                
            # Another train? With larger batch
            self.qplayer.train(batch_size = batch_size, use_last = True, 
                               learn_rate = learn_rate, 
                               reg = reg, reg_rate = reg_rate)
            
            print('Game', game, 'turns', turns - 1, 'counter', counter,
                  'and epsilon', self.epsilon(game))
            
            game += 1
    
    def play_new(self, hgame_class):
        
        assert isinstance(hgame_class, type), 'Argument cgame must be a class (not instance).'
        self.hgame_class = hgame_class
        hgame = self.hgame_class()
        cgame = self.cgame_class()
        
        # Remove initial piece
        game.clear_term()
        print(self)
        
        time.sleep(0.5)
        
        new_state = hgame.get_state(
                        hgame.first_turn(
                            cgame.c_to_h_inpt(
                                self.qplayer.get_move(
                                    hgame.get_state()
                                )
                            )
                        )
                    )
        
        # Do other plays from there
        while not hgame.crash():
            
            self.hgame.clear_term()
            print(self)
            print('\n' + hgame.str_history())
            
            time.sleep(0.5)
            
            new_state = hgame.get_state(
                            hgame.turn(
                                cgame.c_to_h_inpt(
                                    self.qplayer.get_move(
                                        new_state
                                    )
                                )
                            )
                        )
        
        # Finish game
        self.clear_term()
        print(self)
        
        print('Pieces left:', len(self.pieces))
        if len(hgame.pieces) == 1:
            print('\n   Congratulations!\n')
        else:
            print('\n   Shame... Try again!\n')
        print('\nHistory:\n' + self.str_history())
        
    def replay(self, hgame_class, game_ind):
        ''' Play a game from memory. '''
        
        assert isinstance(hgame_class, type), 'Argument cgame must be a class (not instance).'
        self.hgame_class = hgame
        hgame = self.hgame_class()
        cgame = self.cgame_class()
        
        # Remove initial piece
        game.clear_term()
        print(self)
        
        time.sleep(0.5)
        
        new_state = hgame.get_state(
                        hgame.first_turn(
                            cgame.c_to_h_inpt(
                                self.cgames[game_ind].history[0]
                            )
                        )
                    )
        
        # Do other plays from there
        turn = 1
        while not hgame.crash():
            
            self.hgame.clear_term()
            print(self)
            print('\n' + hgame.str_history())
            
            time.sleep(0.5)
            
            new_state = hgame.get_state(
                            hgame.turn(
                                cgame.c_to_h_inpt(
                                    self.cgames[game_ind].history[turn]
                                )
                            )
                        )
            
            turn += 1
        
        # Finish game
        self.clear_term()
        print(self)
        
        print('Pieces left:', len(self.pieces))
        if len(hgame.pieces) == 1:
            print('\n   Congratulations!\n')
        else:
            print('\n   Shame... Try again!\n')
        print('\nHistory:\n' + self.str_history())
        
        pass
        
        
        
        
