## ~~~~~~~~~~~~~~~
#  Deep Q Learning
## ~~~~~~~~~~~~~~~

''' 

Depict game we want to play as a black box. It has a given state, you give it an action and it returns you a reward that depends on both the action and the state. The state changes and you repeat until the game ends (terminal state).

Idea is that the network predicts the Q-value of each action given a state. The Q-value is a sort of value/utility of the action (a) given the state (s):
    Q(s, a) = Sum_t y^t * r_t = r(s, a) + y * max_a'{Q(s', a')}
where r(s, a) is the immediate reward of action a in state s (Bellman equation).

We can thus pick actions by running the network on a given state and choosing the action with highest Q(s, a).

While playing game, we update the training examples and retrain the network to optimise the prediction. We add some random behaviour in order to allow the player to 'explore' the game (otherwise it repeats the actions it thinks are optimal over and over and can get stuck in a local minimum). The idea is that this iteration should make the network's estimate of Q(s, a) near the true Q(s, a).

Steps for running the learning. 
1. Initialise the class instances, including the neural network (input: dim(state space), output: dim(action space));
2. Run game. At each turn:
   a) Run a lottery to see if action will be random or if it will be based on running the network with current weights and choosing action with greatest output (Q-value);
   b) Store old state, action, reward and new state (inc. if it crashed) into the memory: (s, a, r, s', c);
   c) Train network on a single training example generated by:
       - running network on s' (store that) and on s;
       - updating result from running on s corresponding to action a to be r + y * max_a'{Q(s', a')}, store this new result as y
       - train network on single example (x = s, y = y)
3. Train network on larger sample of memory:
   a) Extract batch from memory
   b) Rowwise on memory batch, do the first two steps described in 2c above  to generate a larger training sample
   c) Train network on this sample
4. Go back to step 2. Run game again a set number of times to allow network to learn.

Add in:
 - lottery that decides if move is random or not starts off always giving random and slowly gives less and less
 - variable (slowing) learning rate

'''

import types
import random
import numpy as np

import NeuralNet as NN

## Q-learning player
## ~~~~~~~~~~~~~~~~~

class QPlayer(object):
    ''' Implements things like memory, the deep Q network and training on a batch from memory. '''
    
    def __init__(self, max_memory_len, disc_rate):
        
        self.memory = []
        self.memory_counts = {}
        self.max_memory_len = max_memory_len
        
        self.disc_rate = disc_rate
    
    ''' I should think of what's the best way to program the network itself because the input (board configuration) will have a lot of structure and symmetries that could be expoited, e.g. spatial relation of pieces in board (suggests CNN) and the rotational symmetry of the board (playing it in any orientation is the same - suggests an analogue of CNN where instead of convolving we convolve four rotations of the board? or something like that...). '''
    def start_network(self, sizes, 
                      act_func = NN.Sigmoid, 
                      cost_func = NN.CrossEntropyCost):
        
        self.num_inputs = sizes[0]
        self.num_outputs = sizes[-1]
        
        self.network = NN.NeuralNet(sizes = sizes,
                                    act_func = act_func,
                                    cost_func = cost_func)
        
    def get_move(self, state, random_move, legal_moves):
        ''' Selects random move if random_move == True. Otherwise, runs network to select move with the highest expected (Q-)value. '''
       
        # Random choice
        if random_move:
            return random.choice(legal_moves)
        
        # From network
        preds = self.network.forward_prop([state])[0]
        
        move = np.argmax(preds)
        
        #while move not in legal_moves:    
            #preds = np.delete(preds, move)
            #move = np.argmax(preds)
        
        return int(move)
    
    def store(self, state, action, reward, new_state, crash,
              memory_prop = None):
        
        self.memory.append([state, action, reward, new_state, crash])
        try:
            self.memory_counts[reward] += 1
        except KeyError:
            self.memory_counts[reward] = 1
        
        if self.memory_counts[reward] > memory_prop * self.max_memory_len:
            i = 0
            while self.memory[i][2] != reward:
                i += 1
            del self.memory[i]
        
    def retrieve(self, ind = None):
        
        if ind is None:
            return self.memory
        
        return self.memory[ind]
    
    @staticmethod
    def states_as_string(states, many = True):
        ''' For use in train. Converts states as list to state as string, eg. [1, 1, 0, ...] -> '110...'. '''
        
        if not many:
            
            string_state = ''
            for i in states:
                string_state += str(i)

            return string_state
        
        else:
            
            return np.array([QPlayer.states_as_string(state, False) for state in states])
        
        
    def train(self, batch_size, learn_rate, 
              use_last = True, reg = None, reg_rate = None):
        ''' Trains the network on a batch drawn randomly from memory. If use_last == True, assures the latest training example is included in the batch. '''
        
        # Gen batch for training
        if (batch_size == 1 or len(self.memory) == 1) and use_last:
            batch = np.array([self.memory[-1]])
        elif use_last:
            batch = np.random.permutation(self.memory[:-1])[:batch_size-1]
            batch = np.append(batch, [self.memory[-1]], axis = 0)
        else:
            batch = np.random.permutation(self.memory)[:batch_size]
        
        # Gen 'labels' for training examples in batch
        # Inds: 0 - state, 1 - action, 2 - reward, 3 - new state, 4 - crash
        batch = batch.transpose()
        
        string_new_states = self.states_as_string(batch[3], batch_size > 1)
        
        targets = self.network.forward_prop(np.array(list(batch[0]))).transpose()
        
        for new_state in set(tuple(x) for x in batch[3]):
            
            string_new_state = self.states_as_string(new_state, False)
            
            new_prediction = self.network.forward_prop([list(new_state)])
            max_new_pred = max(new_prediction[0])
            
            for i in range(targets.shape[0]):
                                
                targets[i] = targets[i] + \
                             (string_new_states == string_new_state) * \
                             (batch[1] == i) * \
                             (batch[2] + (batch[4] == False) * self.disc_rate * max_new_pred - targets[i])
        
        targets = targets.transpose()
        
        # Given batch of training examples and targets to train, train
        examples = np.array(list(batch[0]))
        
        self.network.train(X = examples, y = targets,
                           epochs = 1, batch_size = -1,
                           eta = learn_rate,
                           reg = reg, lmbda = reg_rate)
        
        
    


## RL routine
## ~~~~~~~~~~

class RLRoutine(object):
    ''' A class for the Reinforcement learning routine - just a wrapper of a QPlayer and a ComputerGame. Stores the agent and history of games as attributes and has two methods: one for learning and one for playing (in a way humans can visualise). '''
    
    def __init__(self, epsilon, disc_rate, max_memory_len,
                 sizes, act_func = NN.Sigmoid, cost_func = NN.CrossEntropyCost):
        
        # Initialise QPlayer class instance
        self.qplayer = QPlayer(max_memory_len, disc_rate)
        self.qplayer.start_network(sizes, act_func, cost_func)
        
        # Store parameters
        self.epsilon = epsilon          # Function of the epoch
        self.disc_rate = disc_rate
        self.max_memory_len = max_memory_len
        
    def learn(self, cgame_class,
              games, batch_size, learn_rate, 
              memory_props, 
              reg = None, reg_rate = None,
              verbose = False):
        ''' Whole RL routine for learning. 
            1. Run game. At each turn:
               a) Run a lottery to see if action will be random or the actor's network;
               b) Play with that action;
               c) Store old state, action, reward, new state and whether the game crashed in memory: (s, a, r, s', c);
               d) Train network on a small batch generated by sampling from memory and, rowwise:
                   - running network on s' (store that) and on s;
                   - using as target the result from running on s with the Q-value corresponding to a updated to r + y * max_a'{Q(s', a')};
            2. Train network on larger sample of memory using the same method as above;
            3. Go back to step 1. Run game a set number of times to allow network to learn.
        '''
        
        assert isinstance(cgame_class, type), 'Argument cgame must be a class (not instance).'
        self.cgame_class = cgame_class
        self.cgames = []
        
        print('Starting learning.')
        
        game = 0
        counter = 0
        while game < games:
            
            # Start new game
            self.cgames.append(self.cgame_class())
            cgame = self.cgames[-1]
            
            # First turn
            state = cgame.get_state()
            action = int(self.cgame_class.get_inp_out_dims()[1]/2) # <- d4   #self.qplayer.get_move(state, (random.random() < self.epsilon(counter)), range(self.cgame_class.get_inp_out_dims()[1]))
            reward = cgame.first_turn(action)
            new_state = cgame.get_state()
            
            self.qplayer.store(state, action, reward, new_state, cgame.crash(),
                               memory_props[reward])
            
            self.qplayer.train(batch_size = 1, use_last = True, 
                               learn_rate = learn_rate, 
                               reg = reg, reg_rate = reg_rate)
            
            turns = 1
            counter = 1
            
            # Following turns
            while not cgame.crash():
                
                state = new_state
                action = self.qplayer.get_move(state, (random.random() < self.epsilon(counter, game)), cgame.legal_moves())
                
                while action not in cgame.legal_moves():
                    reward = cgame.turn(action)
                    self.qplayer.store(state, action, reward, state, True,
                                       memory_props[reward])
                    self.qplayer.train(batch_size = 1, use_last = True,
                                       learn_rate = learn_rate, 
                                       reg = reg, reg_rate = reg_rate)
                    
                    action = self.qplayer.get_move(state, (random.random() < self.epsilon(counter, game)), cgame.legal_moves())
                    
                    counter += 1
                
                reward = cgame.turn(action)
                new_state = cgame.get_state()
                
                self.qplayer.store(state, action, reward, new_state, cgame.crash(),
                                   memory_props[reward])
                
                self.qplayer.train(batch_size = 1, use_last = True, 
                                   learn_rate = learn_rate, 
                                   reg = reg, reg_rate = reg_rate)
                
                turns += 1
                
            # Another train? With larger batch
            self.qplayer.train(batch_size = batch_size(game+1), use_last = True, 
                               learn_rate = learn_rate, 
                               reg = reg, reg_rate = reg_rate)
            
            print('Game', game, 'turns', turns - 1, 'counter', counter)
            
#            if verbose:
#                if game % verbose == 0:
#                    print('Game ' + str(game) + ', done in ' + str(len(cgame.history)) + ' turns.')
            
            game += 1
    
    def play_new(self, hgame_class):
        
        assert isinstance(hgame_class, type), 'Argument cgame must be a class (not instance).'
        self.hgame_class = hgame_class
        hgame = self.hgame_class()
        cgame = self.cgame_class()
        
        # Remove initial piece
        game.clear_term()
        print(self)
        
        time.sleep(0.5)
        
        new_state = hgame.get_state(
                        hgame.first_turn(
                            cgame.c_to_h_inpt(
                                self.qplayer.get_move(
                                    hgame.get_state()
                                )
                            )
                        )
                    )
        
        # Do other plays from there
        while not hgame.crash():
            
            self.hgame.clear_term()
            print(self)
            print('\n' + hgame.str_history())
            
            time.sleep(0.5)
            
            new_state = hgame.get_state(
                            hgame.turn(
                                cgame.c_to_h_inpt(
                                    self.qplayer.get_move(
                                        new_state
                                    )
                                )
                            )
                        )
        
        # Finish game
        self.clear_term()
        print(self)
        
        print('Pieces left:', len(self.pieces))
        if len(hgame.pieces) == 1:
            print('\n   Congratulations!\n')
        else:
            print('\n   Shame... Try again!\n')
        print('\nHistory:\n' + self.str_history())
        
    def replay(self, hgame_class, game_ind):
        ''' Play a game from memory. '''
        
        assert isinstance(hgame_class, type), 'Argument cgame must be a class (not instance).'
        self.hgame_class = hgame
        hgame = self.hgame_class()
        cgame = self.cgame_class()
        
        # Remove initial piece
        game.clear_term()
        print(self)
        
        time.sleep(0.5)
        
        new_state = hgame.get_state(
                        hgame.first_turn(
                            cgame.c_to_h_inpt(
                                self.cgames[game_ind].history[0]
                            )
                        )
                    )
        
        # Do other plays from there
        turn = 1
        while not hgame.crash():
            
            self.hgame.clear_term()
            print(self)
            print('\n' + hgame.str_history())
            
            time.sleep(0.5)
            
            new_state = hgame.get_state(
                            hgame.turn(
                                cgame.c_to_h_inpt(
                                    self.cgames[game_ind].history[turn]
                                )
                            )
                        )
            
            turn += 1
        
        # Finish game
        self.clear_term()
        print(self)
        
        print('Pieces left:', len(self.pieces))
        if len(hgame.pieces) == 1:
            print('\n   Congratulations!\n')
        else:
            print('\n   Shame... Try again!\n')
        print('\nHistory:\n' + self.str_history())
        
        pass
        
        
        
        
